# Master's Thesis

## Enhancing uncertainty estimation and outlier detection through confidence calibration for out-of-distribution data.  

This repository explores the effectiveness of estimating uncertainty and recognizing outliers. These are critical for generating trustworthy, robust machine learning algorithms or deep neural networks. While machine and deep learning models have demonstrated outstanding performance in various tasks, they frequently struggle with out-of-distribution (OoD) data. In this thesis, we explore and take further the prominent approaches like outlier exporter (OE) and decomposed confidence architecture, using in-distribution and out-distribution data. This method improves and generalizes the model's performance on unseen data.

## Goal of the thesis

The primary goal is to enhance confidence calibration methods in machine learning models and improve anomaly detection methods by using out-of-distribution data with outlier exposure and decomposed confidence.


## Getting Started

## Results

## References

[1] Hendrycks,Dan and Kevin Gimpel: A baselinefordetectingmisclassifiedand
out-of-distribution examplesinneuralnetworks. arXivpreprintarXiv:1610.02136,
2016.

[2] Hendrycks,Dan, MantasMazeika and Thomas Dietterich: Deepanomaly
detectionwithoutlierexposure. arXivpreprintarXiv:1812.04606,2018.

[3] Hsu, Yen-Chang, Yilin Shen, Hongxia Jin and ZsoltKira: Generalizedodin:
Detectingout-of-distributionimagewithoutlearningfromout-of-distributiondata. In
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
pages 10951â€“10960,2020.
